<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- Mirrored from ismir2006.ismir.net/ISMIR06Tutorials.htm by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 23 Jan 2014 11:25:40 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
  <meta content="text/html; charset=ISO-8859-1"
 http-equiv="content-type">
  <title>ISMIR2006Tutorials</title>
</head>
<body>
<h3>ISMIR 2006 Tutorial Information - SUNDAY October 8</h3>


The ISMIR 2006 tutorials will take place at the University of Victoria
campus at the new Engineering and Computer Science building.
A map of the UVic campus showing the building (lower
left corner) can be found at:
<br> 
<a href="http://www.uvic.ca/maps/3dmap.html"> UVic 3D map </a> 
<br> 
There is frequent public transportation from downtown and the Empress to UVic:
<br> 
<a href="http://www.busonline.ca/regions/vic/schedules/map.cfm?p=side.txt&amp;line=4&amp;"> Route 4 </a>  
<br> <a href="http://www.busonline.ca/regions/vic/schedules/map.cfm?p=side.txt&amp;line=14&amp;"> Route 14 </a> 
<br> 
Important: Sunday Oct. 8th also happens to be the Royal Victoria Marathon
so make sure you check the bus detours posted on the webpage above.
These are the two easiest routes. The staff at the Empress will also be able to
help you with arranging for a taxi which would be quite affordable
when shared between 2-4 people.




<p> 
09:00-12:00 <a href="#Computational_Rhythm_Description">Computational
Rhythm Description</a><br>
09:00-12:00 <a href="#User_Interfaces_for_Music_Information">User
Interfaces for Music Information Retrieval</a><br>
14:00-17:00 <a href="#MIR_for_audio_signals_using_Marsyas-0.2">MIR for
audio signals using Marsyas-0.2</a><br>
14:00-17:00 <a href="#Bayesian_Methods_for_Music_Signal">Bayesian
Methods for Music Signal Analysis
</a><br>
<br>
<br>
<span style="font-weight: bold;"><a
 name="Computational_Rhythm_Description"></a>Computational Rhythm
Description<br>
</span><span style="font-style: italic;">Fabien Gouyon, Simon Dixon
</span><br style="font-style: italic;">
09:00-12:00<br>
<br>
<span style="font-weight: bold;">Description</span><br>
This tutorial will provide an overview of past and current approaches
<br>
to the automatic description of musical rhythm of audio and MIDI
<br>
files. After defining basic notions of interest, we will propose a
<br>
general functional framework for the analysis and qualitative
<br>
comparison of existing rhythm description systems and review the
<br>
architecture of many existing systems with respect to individual
<br>
blocks of this framework. We will then address the issue of system
<br>
evaluation and report on results from the last two MIREX efforts in
<br>
tempo estimation. We will finally illustrate the use of rhythmic
<br>
descriptors in music content processing applications (from retrieval
<br>
to transformations) and highlight current promising research trends.
<br>
<br>
In this tutorial, a special focus will be put on building bridges
<br>
between high-level music-theoretic rhythm concepts and acoustic
<br>
features of a lower level of abstraction.
<br>
<br>
Rhythm is ubiquitous in the literature on automatic music description,
<br>
notably in the proceedings of past ISMIR conferences. However, when
<br>
including rhythmic features in MIR systems, researchers usually use
<br>
rather low-level representations of rhythm. This tutorial will review
<br>
previous and current attempts at automatic rhythm description of audio
<br>
and MIDI files, illustrate current use of rhythmic features in MIR
<br>
systems and foster the use of higher level rhythmic features for tasks
<br>
such as genre classification and music similarity.<br>
<br>
<span style="font-weight: bold;">Bios</span><br
 style="font-weight: bold;">
<br>
Fabien Gouyon works at the Austrian Research Institute for Artificial
<br>
Intelligence in Vienna. He received his PhD in Computer Science and
<br>
Digital Communication from University Pompeu Fabra in Barcelona. His
<br>
thesis titled "A Computational Approach to Rhythm Description - Audio
<br>
Features for the Computation of Rhythm Periodicity Functions and their
<br>
use in Tempo Induction and Music Content Processing" was supervised by
<br>
Xavier Serra (Pompeu Fabra University, Barcelona) and Gerhard Widmer
<br>
(Johannes Kepler University, Linz). He is member of the Audio
<br>
Engineering Society committee on Semantic Audio Analysis, has
<br>
published more than 30 scientific papers on the computational analysis
<br>
of musical rhythm, onset detection and percussion sound
<br>
classification, he serves as reviewer for several international
<br>
journals and organized the first tempo induction contest at the ISMIR
<br>
2004 conference.
<br>
<br>
Simon Dixon is a research scientist at the Austrian Research Institute
<br>
for Artificial Intelligence (OFAI) in Vienna. He studied computer
<br>
science at the University of Sydney, obtaining the BSc (1989) and PhD
<br>
(1994) degrees, with a dissertation on computational belief revision.
<br>
During his undergraduate studies, he also obtained the AMusA and LMusA
<br>
diplomas in classical guitar. After lecturing in computer science at
<br>
Flinders University of South Australia for 5 years, he moved to Vienna
<br>
in 1999 to join the Intelligent Music Processing and Machine Learning
<br>
Group at OFAI. His research interests focus on the extraction and
<br>
processing of musical content (particularly rhythmic content) in audio
<br>
signals, and he has published over 40 papers covering areas such as
<br>
tempo induction, beat tracking, onset detection, automated
<br>
transcription, genre classification and the measurement and
<br>
visualization of expression in music performance.
<br>
<br>
<br>
<br>
<span style="font-weight: bold;"><a
 name="User_Interfaces_for_Music_Information"></a>User Interfaces for
Music Information Retrieval<br>
</span><span style="font-style: italic;">David Gerhard </span><br
 style="font-style: italic;">
09:00-12:00<br>
<br>
<br>
As in many other disciplines, there is somewhat of a gap in the Music
<br>
Information Retrieval community between those who develop tools and
<br>
those who use the tools.&nbsp; Often, a developer will build a tool to
a
<br>
level of detail sufficient to solve a particular problem and then make
<br>
the tool available to other users.&nbsp; Unless these users are
familiar
<br>
with the development process for the tool, or the underlying parameter
<br>
design, they may have difficulty using the tool.&nbsp; Researchers
<br>
unfamiliar with programming languages may have difficulty using many
<br>
of the available MIR tools, and may become frustrated knowing that
<br>
extracting a particular piece of information is possible if only they
<br>
could figure out how to do it.
<br>
<br>
This tutorial will examine some of the principles of user interface
<br>
design, and how they apply to MIR situations.&nbsp; Presentation of MIR
<br>
data in visualizations, augmented audio, and score notation will be
<br>
discussed, as well as the design of efficient, effective and
<br>
satisfying interactions. Physical interfaces and interface protocols
<br>
will also be discussed, considering their limitations, advantages, and
<br>
the possibility of using traditional interface modes for novel
<br>
interaction paradigms.
<br>
<br>
<span style="font-weight: bold;">Bio</span><br>
<br>
David Gerhard is an Assistant Professor of Computer Science at the
<br>
University of Regina in Saskatchewan, Canada.&nbsp; He received his
B.Sc.
<br>
in Computer Engineering from the University of Manitoba in 1996, and
<br>
in 2003 received his Ph.D. in Computer Science from Simon Fraser
<br>
University for his work detailing the computational differences
<br>
between talking and singing. Since then, he has been studying
<br>
interactive media, specifically computational and human aspects of
<br>
audio interaction, applying techniques from HCI, multimedia, and
<br>
pattern recognition.&nbsp; He has published work related to multimedia
<br>
composition, sound spatialization, and usability of new music devices,
<br>
as well as continuing his work on the analysis of the sung voice. Dr.
<br>
Gerhard is a founding member and the current director of the aRMADILo
<br>
(Rough Music and Audio Digital Interaction Lab) and is an associate
<br>
member of the department of music.
<br>
<span class="moz-txt-tag"></span><br>
<br>
<br>
<span style="font-weight: bold;"><a
 name="MIR_for_audio_signals_using_Marsyas-0.2"></a>MIR for audio
signals using Marsyas-0.2<br>
</span><span style="font-style: italic;">George Tzanetakis and Luis
Gustavo Martins</span><br style="font-style: italic;">
14:00-17:00 <br>
<br>
Marsyas-0.2 is an open source audio processing framework with specific <br>
emphasis on MIR applications. It is written in C++ and provides a large
<br>
number of building blocks for constructing MIR systems. It attempts to <br>
provide high-level expressive abstractions without sacrificing
efficiency. <br>
The tutorial will cover the basics of using the latest rewrite of
Marsyas <br>
(0.2.x releases)&nbsp; with specific examples from audio MIR research
such <br>
as feature extraction, similarity retrieval, classification,
clustering, <br>
segmentation. In addition it will cover more advanced topics such as
the <br>
creation of user interfaces and the interaction of analysis and
synthesis.<br>
<br>
<span style="font-weight: bold;"><br>
Bios<br>
<span style="font-weight: bold;"><br>
</span></span>George Tzanetakis is an assistant Professor of Computer
Science<br>
(cross-listed in Music) at the University of Victoria. He received his<br>
PhD degree in Computer Science from Princeton Uiversity in May 2002<br>
and was a PostDoctoral Fellow at Carnegie Mellon University working on<br>
query-by-humming systems with Prof. Dannenberg and on video retrieval<br>
with the Informedia group. In addition he has worked as a summer<br>
intern at SRI on multimedia browsing user interfaces, was chief<br>
designer of the audio fingerprinting technology of Moodlogic Inc., and<br>
developed a real-time music speech classification system for All Music<br>
Publishing, The Netherlands. His research deals with all stages of
audio <br>
content analysis such as feature extraction, segmentation,
classification <br>
with specific focuson Music Information Retrieval (MIR). His
pioneering&nbsp; <br>
work on musical genre classification is frequently cited and received
an <br>
IEEE Signal Processing Society Young Author Award in 2004. He has <br>
presented tutorials on MIR and audio feature extraction at several <br>
international conferences. He is also an active musician and has
studied <br>
saxophone performance, music theory and composition. More<br>
information can be found at: http://www.cs.uvic.ca/~gtzan.<span
 style="font-weight: bold;"><span style="font-weight: bold;"><br>
<span style="font-weight: bold;"></span><br>
</span></span>Luis Gustavo Martins is a PhD student and researcher at
the Audio Group <br>
of the Telecommunications and Multimedia Unit of INESC Porto, Porto,
Portugal. <br>
His main work interests are in the areas of Digital Audio Processing <br>
and Semantic Audio Analysis. He has been involved with the development <br>
of Marsyas for several years and is currently working on implementing <br>
his Masters thesis work on polyphonic music transcription in
Marsyas-0.2. <br>
<br>
<br>
<span style="font-weight: bold;"><a
 name="Bayesian_Methods_for_Music_Signal"></a>Bayesian Methods for
Music Signal Analysis
</span><br>
<span style="font-style: italic;">A. Taylan Cemgil
</span><span style="font-style: italic;"></span><br
 style="font-style: italic;">
14:00-17:00 <br>
<br>
In the last years, there have been a significant growth of music
<br>
information processing applications that employ ideas from statistical
<br>
machine learning and probabilistic modeling.&nbsp; In this paradigm,
music
<br>
data is viewed as realizations from highly structured stochastic
<br>
processes.&nbsp; Once a model is constructed, several interesting
problems
<br>
such as pitch detection, transcription, tempo tracking, source
<br>
separation e.t.c. can be formulated as Bayesian inference problems. In
<br>
this context, graphical models provide a "language" to construct
<br>
models of music such as quantification of prior knowledge about
<br>
physical properties of sound, musical structure and its relation to
<br>
the performance. Unknown parameters in this specification are
<br>
estimated by probabilistic inference. Often, however, the problem size
<br>
poses an important challenge and in order to render the approach
<br>
feasible, specialized inference methods need to be tailored to improve
<br>
the computational speed and efficiency.
<br>
<br>
The scope of the proposed tutorial is as follows: First, we will
<br>
review the fundamentals of probabilistic models of music, both low
<br>
level signal models and high level structure models such as tempo,
<br>
rhythm and harmony. Then, we will discuss the numerical techniques for
<br>
inference in these models. In particular, we will review exact
<br>
inference, approximate stochastic inference techniques such as Markov
<br>
Chain Monte Carlo, Sequential Monte Carlo and deterministic inference
<br>
(variational) techniques. Our ultimate aim is to provide a basic
<br>
understanding of probabilistic modeling for music processing, and a
<br>
roadmap such that music information retrieval researchers new to the
<br>
Bayesian approach can orient themselves in the relevant literature,
<br>
understand the current state of the art and eventually incorporate
<br>
these powerful techniques into their own research.
<br>
<font face="Arial, Helvetica, sans-serif"><br>
</font><span style="font-weight: bold;">Bio<br>
<br>
</span>A. Taylan Cemgil received his B.Sc. and M.Sc.&nbsp; in Computer
<br>
Engineering, Bogazici University, Turkey and his Ph.D. (2004) from
<br>
Radboud University Nijmegen, the Netherlands with a thesis entitled
<br>
Bayesian music transcription. Between 2003-2005, he worked as a
<br>
postdoctoral researcher at the University of Amsterdam on vision based
<br>
multi object tracking.&nbsp; He is currently a research associate at
the
<br>
Signal Processing and Communications Lab., University of Cambridge,
<br>
UK, where he cultivates his interests in machine learning methods,
<br>
stochastic processes and statistical signal processing.&nbsp; His
research
<br>
is focused towards developing theoretically sound computational
<br>
techniques to equip computers with musical listening and interaction
<br>
capabilities, which he believes is essential in construction of
<br>
intelligent music systems and virtual music instruments that can
<br>
listen, imitate and autonomously interact with humans.
<br>
<span style="font-weight: bold;"><span style="font-weight: bold;"><br>
</span></span>
</body>

<!-- Mirrored from ismir2006.ismir.net/ISMIR06Tutorials.htm by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 23 Jan 2014 11:25:40 GMT -->
</html>
